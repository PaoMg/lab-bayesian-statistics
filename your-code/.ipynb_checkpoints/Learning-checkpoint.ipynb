{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Statistics\n",
    "\n",
    "\n",
    "Lesson Goals\n",
    "\n",
    "    Understand how conditional probabilities form the basis for Bayesian statistics.\n",
    "    Introduce Bayes' Theorem and its components.\n",
    "    Use Bayes' Theorem to arrive at an estimate.\n",
    "    Perform a Bayesian analysis using a real-world scenario.\n",
    "\n",
    "Introduction\n",
    "\n",
    "In the Introduction to Probability lesson, we introduced conditional probability, which is a prerequisite for Bayesian Statistics.\n",
    "\n",
    "Conditional Probabilities\n",
    "\n",
    "A conditional probability is a probability based on some background information. It's notation is typically expressed as P(A|B), which means the probability of event A occurring given that some condition B is true. We saw that this could be computed using the following equation.\n",
    "\n",
    "P(A|B) = P(A∩B) / P(B)\n",
    "\n",
    "In other words, the probability of A given B equals the probability of the intersection of A and B divided by the probability of B.\n",
    "\n",
    "We also briefly touched on the independence and dependence of events and said that when events are independent, at least one of the following must be true.\n",
    "\n",
    "P(A|B) = P(A)\n",
    "P(B|A) = P(B)\n",
    "P(A∩B) = P(A) P(B)\n",
    "\n",
    "In other words, if the events are truly independent, then the probability of one given another should just be the probability of that event occurring. Similarly, the probability of the intersection of the two events should just be the product of the probabilities of the two events, since neither one impacts the other.\n",
    "Bayes' Theorem\n",
    "\n",
    "For dependent events, we need to modify the last set of equations to account for the fact that one event impacts the other.\n",
    "\n",
    "P(A∩B) = P(A) P(B|A)\n",
    "P(B∩A) = P(B) P(A|B)\n",
    "\n",
    "So the intersection of A and B equals the probability of A times the probability of B given A and, since there is nothing special about A versus B, it also equals the probability of B times the probability of A given B.\n",
    "\n",
    "If this is the case, then:\n",
    "\n",
    "P(B) P(A|B) = P(A) P(B|A)\n",
    "\n",
    "And if we divide both sides by P(B), we end up with the following.\n",
    "\n",
    "P(A|B) = P(A) P(B|A) / P(B)\n",
    "\n",
    "This equation is known as Bayes' Theorem, and it is a surprisingly powerful equation that forms the fundamental idea behind all Bayesian statistics. It allows us to establish what we believe about the probability of a hypothesis prior to seeing any data, and it also provides us with a means by which to update those beliefs once we have seen some data.\n",
    "\n",
    "This equation has a few different components, so let's break those down:\n",
    "\n",
    "    P(A) is known as the prior probability, and it is the the probability of the hypothesis before we see the data.\n",
    "    \n",
    "    P(B|A) is known as the likelihood, and it represents the probability of the data under the hypothesis.\n",
    "    \n",
    "    P(B) is known as the marginal probability, and it is a normalizing constant that represents the probability of the data under any hypothesis, or P(A∪B). In other words, it can be computed as P(A) P(B|A) + P(-A) P(B|-A) where -A is Not A (the alternative scenario of the hypothesis).\n",
    "    \n",
    "    P(A|B) is known as the posterior probability, and it is the probability that we want to compute after having seen the data.\n",
    "\n",
    "Important: It is important to emphasize here that the hypothesis is represented by A and the observed data is represented by B. A common mistake and point of confusion is to think that A and B represent two different hypotheses, which is not the case. The two opposing hypotheses are represented by A and -A.\n",
    "\n",
    "We can define a function in Python that accepts a list of priors and corresponding likelihoods. Our function will compute the marginal (marg) and then use Bayes' Theorem to calculate the posterior probabilities for the each of the hypotheses (post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bayes_rule(priors, likelihoods):\n",
    "    #marg = normalizing constant, va a dar un número que es la prob de la data bajo cualquier hipótesis\n",
    "    marg = sum(np.multiply(priors, likelihoods))\n",
    "    #post = una lista de posibles posteriori probabilidades ya conociendo la data\n",
    "    post = np.divide(np.multiply(priors, likelihoods), marg)\n",
    "    return post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Bayes' Theorem\n",
    "\n",
    "Now that we have our function, let's propose a scenario and use our function to help us solve for the posterior probabilities.\n",
    "\n",
    "Suppose we have 3 jars with 100 marbles in each.\n",
    "\n",
    "    Jar 1 has 40 blue marbles, 30 red marbles, and 30 green marbles.\n",
    "    Jar 2 has 60 blue marbles, 20 red marbles, and 20 green marbles.\n",
    "    Jar 3 has 10 blue marbles, 30 red marbles, and 60 green marbles.\n",
    "\n",
    "You pick a marble from one of the jars at random, and the marble is green. What are the probabilities that the marble came from each of the three jars?\n",
    "\n",
    "To answer this question, we would need to know what our prior probabilities were and our likelihoods. For the priors, we can assume that the probability we drew the marble from any one jar is equally probably, or 1/3. The likelihoods in this case would just be the percentages of green marbles in each jar. We can plug these values into our bayes_rule function, and it will return the following outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27272727, 0.18181818, 0.54545455])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors = [1/3, 1/3, 1/3]\n",
    "likelihoods = [0.3, 0.2, 0.6]\n",
    "\n",
    "bayes_rule(priors, likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can see that there is a 25% chance that the green marble we drew came from Jar 1, a 16.67% chance that it came from Jar 2, and a 58.3% chance that it came from Jar 3.\n",
    "\n",
    "Suppose you had picked a blue marble instead of a green one. We would just need to update our likelihoods to account for the proportion of each jar whose marbles were blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36363636, 0.54545455, 0.09090909])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihoods = [0.4, 0.6, 0.1]\n",
    "\n",
    "bayes_rule(priors, likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a 36.36% chance that the green marble we chose came from Jar 1, a 54.54% chance it came from Jar 2, and only a 9% chance that it came from Jar 3.\n",
    "\n",
    "\n",
    "# Bayesian Data Analysis\n",
    "\n",
    "The previous example was a simple one with the objective of demonstrating how Bayes' Theorem works. When doing Bayesian Data Analysis in the real world, the priors and the posteriors are typically probability distributions. We typically have some data, and the way to get from the prior to the posterior is typically a generative model (one that accepts parameters and generates data from them).\n",
    "\n",
    "To get a sense of how this works, let's look at an example. Suppose we have just launched an online store that sells fidget spinners, and we are trying to estimate what percentage of visitors to our online store will make a purchase. We can set this up as a Bayesian problem and arrive at a probability distribution as follows.\n",
    "\n",
    "Prior to seeing any traffic or purchase data, we have no idea what to expect so we will start with the assumption that all percentages are equally likely. This is called a uniform prior because we can use a uniform distribution to represent the fact that every rate has an equal chance of being the actual rate of purchase. To set this up in Python, we are going to use Numpy to generate a uniform distribution from 100,000 random draws of numbers between 0 and 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.219966\n",
       "1        0.289479\n",
       "2        0.446130\n",
       "3        0.509106\n",
       "4        0.962838\n",
       "           ...   \n",
       "99995    0.540630\n",
       "99996    0.543362\n",
       "99997    0.591765\n",
       "99998    0.161482\n",
       "99999    0.851610\n",
       "Length: 100000, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n_draws = 100000\n",
    "#prior nos da una lista de 100,000 posibles probabilidades entre 0 y 1 (de que compren en la tienda)\n",
    "prior = pd.Series(np.random.uniform(0, 1, size=n_draws))\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our prior distribution, and it looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prior.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to observe some data and see how this affects our estimates. Suppose on the first day, 50 people visited our site and 10 of them made a purchase. We are going to record the number of purchases in a variable called observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVADO: DE 50 VISITAS, 10 COMPRARON \n",
    "observed = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ware also going to create a generative model that will randomly draw from our prior uniform distribution, simulate 50 people coming to our website a large number of time, and see how many times we get a result that is in line with the result we have observed. To do this in Python, we are going to define a generative_model function that accepts a random probability parameter from our prior and then performs 50 binomial draws using that probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La funcion generative_model nos va a SIMULAR a las 50 personas y vamos a ver de esas 50, cuantas arroja que \n",
    "#compraron\n",
    "#result va a ser una lista de 100,000 números, que nos va a decir en cada día de 50, cuantos compraron\n",
    "def generative_model(param):\n",
    "    result = np.random.binomial(50, param)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a empty list that is going to contain our simulated results (sim_data) and then populate it by appending the results when each value in our prior distribution is plugged into our generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = list()\n",
    "\n",
    "for p in prior:\n",
    "    sim_data.append(generative_model(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are going to arrive at our posterior distribution by selecting only the values from our prior distribution that generated instances where the simulated result matched our observed result of 10 purchases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83       0.279169\n",
       "112      0.246789\n",
       "145      0.251911\n",
       "294      0.167332\n",
       "347      0.216319\n",
       "           ...   \n",
       "99885    0.155489\n",
       "99924    0.208563\n",
       "99941    0.252333\n",
       "99960    0.274067\n",
       "99986    0.350677\n",
       "Length: 2036, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = prior[list(map(lambda x: x == observed, sim_data))]\n",
    "posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what our posterior distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "posterior.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see just by looking at this that the updated probability of someone making a purchase from our online store is likely between 10% and 30% and most likely somewhere between 17% and 20%.\n",
    "\n",
    "We can also calculate some statistics for our posterior distribution in addition to visualizing it. This will show us the mean, range, and standard deviation of our posterior distribution in addition to the quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1969.000000\n",
       "mean        0.210563\n",
       "std         0.056635\n",
       "min         0.066313\n",
       "25%         0.171937\n",
       "50%         0.207019\n",
       "75%         0.247364\n",
       "max         0.418641\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both the mean and the median are approximately 21%. If we wanted to, we could also calculate a 90% credible interval for this distribution as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10741278242768902 | 0.32880005303193843\n"
     ]
    }
   ],
   "source": [
    "print(posterior.quantile(.025), '|', posterior.quantile(.975))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could also round the values in the posterior distribution to whole percents and calculate the most likely one along with its probability. This is known as the maximum likelihood estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Likelihood Estimate:  0.2 | 0.07668867445403758\n"
     ]
    }
   ],
   "source": [
    "rounded = posterior.round(2)\n",
    "mode = rounded.mode()[0]\n",
    "probability = list(rounded).count(mode)/len(rounded)\n",
    "print('Maximum Likelihood Estimate: ', mode, '|',probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that given the data we have thus far, it is most likely that 18% of the visitors to our online store will make a purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
